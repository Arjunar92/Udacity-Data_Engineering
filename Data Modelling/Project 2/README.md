# Data Modeling with Postgres

## Introduction

Sparkfy, a new online music streaming app platform wants to analyze their user activity in their music app. Currently the data is not in a form that can be easily queried for information. 
They would like a Data Engineer to suggest them on possible solutions, create database schemas and build an ETL pipeline. 

## Project objective:

Use Postgres database to build a ETL pipeline using Python. Define fact and Dimension tables, implement a star schema. the data needs to be extracted from the two JSON files, transformed using pipelines and then ultimately load to the database. 


## File info:

### 1. data:

- song_data: Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are file paths to two files in this dataset.

- log_data:  This consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

### 2. etl.py: 
- Extract data from song_data and log_data and load them to a database. 

### 3. sql_queries.py:
- Has helper SQl query statements to CREATE, INSERT, SELECT, DROP tables, for etl.py and create_tables.py

### 4. create_tables.py:
Run this file to create Fact and Dimensions table. 

- create_database: This function helps in dropping existing database, create new database and return the connection.

- drop_tables: Used to drop the existing tables.

- create_tables: This helps in creating above mentioned fact table and dimension tables.

### 5. test.ipynb
- run this to check if the database tables are created successfully. 

### 6. etl.ipynb
- Jupyter notebook to create the initial pipeline, later to be implemented in etl.py


## Database Schemas

![image](https://user-images.githubusercontent.com/35266145/152668627-10e97c30-19ce-4152-83f8-ba08f2b1a6e8.png)


### Fact table 
        songplays
        - songplay_id PRIMARY KEY,
        - start_time timestamp
        - user_id
        - level
        - artist_id
        - song_id
        - session_id
        - location
        - user_agent 

### Dimension table
        users
        - user_id PRIMARY KEY
        - first_name
        - last_name
        - gender
        - level

        songs
        - song_id PRIMARY KEY
        - title
        - artist_id
        - year
        - duration

        artists
        - artist_id PRIMARY KEY
        - artist_name
        - artist_location
        - artist_latitude
        - artist_longitude

        time
        - start_time PRIMARY KEY
        - hour
        - day
        - week
        - month
        - year
        - weekday
        
## Pipeline execution. 

Run the below two python file each time before running the pipeline.

    - python create_tables.py

    - python etl.py
